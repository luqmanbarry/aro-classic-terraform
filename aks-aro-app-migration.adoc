= AKS to ARO application migration - WIP

== Pre-requisites
* Azure permission to:
** Create/Delete Azure Disks
** Read Storage Account & Container
* Azure Service Principal (used by Velero in ARO) that has read access to the AKS Storage Container hosting the Velero backups
* Admin access to an AKS cluster
* Admin access to an ARO cluster
* CLI Programs: https://velero.io/docs/v1.8/basic-install/#install-the-cli[velero], https://docs.openshift.com/container-platform/4.16/cli_reference/openshift_cli/getting-started-cli.html[openshift-client], https://helm.sh/docs/intro/install/[helm], https://jqlang.github.io/jq/download/[jq]
* Network connectivity:
** Bastion to AKS
** Bastion to ARO

== Procedure

We start from a stage where Velero has been periodically taking AKS application backups. Velero Backup stores Kubernetes objects manifests such as Secrets, ConfigMaps, Deployments... inside a storage container, and take disks snapshots of application persistent state (data). 

We are primarily going to look at how to setup Velero on ARO, and run Velero Restore.

. https://velero.io/docs/v1.8/basic-install/#install-the-cli[Install Velero] on the target (ARO) cluster
+
.. Configure StorageClass mapping
+
  Prepare the ConfigMap manifest and apply it on the cluster.
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  # any name can be used; Velero uses the labels (below)
  # to identify it rather than the name
  name: change-storage-class-config
  # must be in the velero namespace
  namespace: velero
  # the below labels should be used verbatim in your
  # ConfigMap.
  labels:
    # this value-less label identifies the ConfigMap as
    # config for a plugin (i.e. the built-in change storage
    # class restore item action plugin)
    velero.io/plugin-config: ""
    # this label identifies the name and kind of plugin
    # that this ConfigMap is for.
    velero.io/change-storage-class: RestoreItemAction
data:
  # add 1+ key-value pairs here, where the key is the old
  # storage class name and the value is the new storage
  # class name.
  <source-cluster-storage-class>: <target-cluster-storage-class>
----
+
Save the manifest to a file named `aks-2-aro-storage-class-mapping.yaml` and apply it
+
[source,sh]
----
oc apply -f aks-2-aro-storage-class-mapping.yaml
----
+
.. Check which backups are available for restore
+
[source,sh]
----
oc get backups -n velero
----
+
. Optionally apply the privileged SecurityContextConstraints CR onto the ARO cluster
+
Applications running on AKS often use privileged pods. During the migration, create a privileged SecurityContextConstraints and add your apps ServiceAccounts to it to avoid being bugged down on troubleshooting SecurityContext related issues. However, after the migration one should fix the applications so that they  do not need to run as privileged.
+
.. Prepare the SecurityContextConstraints CR
+
[source,yaml]
----
allowHostDirVolumePlugin: true
allowHostIPC: true
allowHostNetwork: true
allowHostPID: true
allowHostPorts: true
allowPrivilegedContainer: true
allowedCapabilities: 
- '*'
apiVersion: security.openshift.io/v1
defaultAddCapabilities: [] 
fsGroup: 
  type: RunAsAny
groups: 
- system:cluster-admins
- system:nodes
kind: SecurityContextConstraints
metadata:
  annotations:
    kubernetes.io/description: 'privileged allows access to all privileged and host
      features and the ability to run as any user, any group, any fsGroup, and with
      any SELinux context.  WARNING: this is the most relaxed SCC and should be used
      only for cluster administration. Grant with caution.'
  creationTimestamp: null
  name: aks-migration-scc
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities: 
- KILL
- MKNOD
- SETUID
- SETGID
runAsUser: 
  type: RunAsAny
seLinuxContext: 
  type: RunAsAny
seccompProfiles:
- '*'
supplementalGroups: 
  type: RunAsAny
users: 
- system:serviceaccount:nexus:nexus
- system:serviceaccount:sonarqube:sonarqube-sonarqube
- system:serviceaccount:nexusiq:nexusiq
- system:serviceaccount:nexusiq:nexusiq-postgresql
- system:serviceaccount:jenkins:jenkins
volumes: 
- '*'
----
+
Save the manifest to a file named `aks-migration-scc.yaml` and apply it
+
[source,sh]
----
oc apply -f aks-migration-scc.yaml
----
+
. Ensure AKS disks SKU matches that of ARO (ie: LRS --> ZRS)
+
This applies to every application within the migration scope.
+
.. Identity the application disk by describing the PersistentVolume bound to the application's PersistentVolumeClaim
+
[source,sh]
----
oc get persistentvolumeclaim <pvc_name>
oc describe persistentvolume <pv_name>
# Disk info will be in the describe output
----
+
.. Scale down the AKS application
.. https://learn.microsoft.com/en-us/azure/virtual-machines/disks-convert-types?tabs=azure-portal[Change the AKS disk SKU]
.. Scale up the AKS application
. Take fresh backups if the application's PersistentVolume disk SKU was modified
+
[source,sh]
----
velero backup create <BACKUP-NAME> --include-namespaces <NAMESPACE1> [--default-volumes-to-restic]
----
+
.. Check the backup progress
+
[source,sh]
----
velero backup describe <BACKUP-NAME>
----
+
. Change kube context to the ARO cluster and run Velero restore
+
Ensure application dependencies such as other apps, cluster configurations..etc are present on the cluster before initiating the restore.
+
.. Make sure the Velero backup object exists and is healthy by running the following command
+
[source,sh]
----
velero backup describe <BACKUP-NAME>
----
+
.. Execute velero restore
+
[source,sh]
----
velero restore create --from-backup <BACKUP-NAME>
----
+
.. Check the restore progress
+
[source,sh]
----
velero restore describe <BACKUP-NAME>
----